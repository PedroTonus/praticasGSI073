{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroTonus/praticasGSI073/blob/main/Aula02_tokenizacao_pratica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 2 - Tokeniza√ß√£o"
      ],
      "metadata": {
        "id": "R8ElgxbDEATe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "846092fa"
      },
      "source": [
        "sentences = [\n",
        "\"O gato observou o p√°ssaro na janela\",\n",
        "\"O menino chutou a bola no quintal\",\n",
        "\"A chuva molhou a rua inteira\",\n",
        "\"O vento balan√ßou as √°rvores altas\",\n",
        "\"A m√£e preparou o caf√© da manh√£\",\n",
        "\"O cachorro encontrou um graveto grande\",\n",
        "\"A professora explicou a tarefa dif√≠cil\",\n",
        "\"O piloto guiou o avi√£o suavemente\",\n",
        "\"A crian√ßa abra√ßou o ursinho favorito\",\n",
        "\"O artista pintou um quadro colorido\",\n",
        "\"A lua iluminou o campo escuro\",\n",
        "\"O m√∫sico afinou o viol√£o antigo\",\n",
        "\"A menina leu um livro interessante\",\n",
        "\"O carteiro entregou uma carta urgente\",\n",
        "\"A l√¢mpada clareou o quarto frio\",\n",
        "\"O surfista enfrentou uma onda gigante\",\n",
        "\"A bailarina praticou um salto perfeito\",\n",
        "\"O escritor iniciou um novo cap√≠tulo\",\n",
        "\"A estrela brilhou no c√©u limpo\",\n",
        "\"O trem atravessou o t√∫nel longo\",\n",
        "\"A borboleta pousou na flor vermelha\",\n",
        "\"O gato bebeu √°gua fresca\",\n",
        "\"A m√©dica examinou o paciente nervoso\",\n",
        "\"O turista tirou fotos da montanha\",\n",
        "\"A planta cresceu rapidamente na varanda\",\n",
        "\"O menino encontrou uma moeda dourada\",\n",
        "\"A menina desenhou um castelo bonito\",\n",
        "\"O pescador lan√ßou a rede no lago\",\n",
        "\"A neve cobriu a estrada vazia\",\n",
        "\"O garoto abriu um pacote misterioso\",\n",
        "\"A cantora ensaiou uma melodia suave\",\n",
        "\"O marceneiro cortou a madeira grossa\",\n",
        "\"A vela iluminou o corredor sombrio\",\n",
        "\"O beb√™ segurou o dedo da m√£e\",\n",
        "\"A bicicleta atropelou uma po√ßa de √°gua\",\n",
        "\"O carteiro encontrou o port√£o fechado\",\n",
        "\"A sopa esquentou a cozinha fria\",\n",
        "\"O trem apitou na esta√ß√£o central\",\n",
        "\"A √°rvore abrigou v√°rios p√°ssaros\",\n",
        "\"O aluno terminou o exerc√≠cio r√°pido\",\n",
        "\"A nuvem bloqueou o sol forte\",\n",
        "\"O gato arranhou o sof√° novo\",\n",
        "\"A girafa alcan√ßou as folhas altas\",\n",
        "\"O menino soprou uma bolha enorme\",\n",
        "\"A mulher guardou as roupas limpas\",\n",
        "\"O rel√≥gio marcou meia-noite\",\n",
        "\"O le√£o rugiu para o horizonte\",\n",
        "\"A menina escondeu um bilhete secreto\",\n",
        "\"O pintor limpou seus pinc√©is velhos\",\n",
        "\"A tartaruga atravessou o gramado\",\n",
        "\"O avi√£o sobrevoou a cidade grande\",\n",
        "\"A atriz decorou um texto dif√≠cil\",\n",
        "\"O cachorro cavou um buraco fundo\",\n",
        "\"A porta rangiu com o vento\",\n",
        "\"O garoto segurou o chap√©u vermelho\",\n",
        "\"A av√≥ preparou um bolo cheiroso\",\n",
        "\"O gato pulou sobre a cadeira\",\n",
        "\"A abelha pousou na p√©tala rosa\",\n",
        "\"O menino dobrou o papel cuidadosamente\",\n",
        "\"A m√∫sica encheu o sal√£o vazio\",\n",
        "\"O sol aqueceu as pedras da praia\",\n",
        "\"A menina encontrou um colar brilhante\",\n",
        "\"O homem fechou a janela depressa\",\n",
        "\"A moto passou pela rua estreita\",\n",
        "\"O cavalo correu pelo campo verde\",\n",
        "\"A crian√ßa segurou um bal√£o azul\",\n",
        "\"O jardineiro cortou a grama alta\",\n",
        "\"O motorista virou na esquina errada\",\n",
        "\"A bailarina amarrou as sapatilhas\",\n",
        "\"O peixe nadou entre as algas\",\n",
        "\"A garota escreveu uma carta longa\",\n",
        "\"O atleta correu para a linha final\",\n",
        "\"A porta bateu com for√ßa\",\n",
        "\"O professor recolheu os trabalhos\",\n",
        "\"O gato derrubou um vaso pequeno\",\n",
        "\"A gaivota sobrevoou a praia cheia\",\n",
        "\"O garoto perdeu o l√°pis preto\",\n",
        "\"A menina cuidou da planta fr√°gil\",\n",
        "\"O carro freou na faixa de pedestres\",\n",
        "\"A chuva refrescou a tarde quente\",\n",
        "\"O ator improvisou uma fala engra√ßada\",\n",
        "\"A bandeira balan√ßou ao vento\",\n",
        "\"O cachorro encontrou um osso velho\",\n",
        "\"A m√£e organizou os brinquedos espalhados\",\n",
        "\"O violinista tocou uma nota longa\",\n",
        "\"A estrela guiou os viajantes cansados\",\n",
        "\"O detetive analisou uma pista nova\",\n",
        "\"A crian√ßa escondeu o doce favorito\",\n",
        "\"O vento moveu a cortina leve\",\n",
        "\"A mar√© trouxe conchas coloridas\",\n",
        "\"O telefone tocou inesperadamente\",\n",
        "\"A menina abra√ßou a amiga triste\",\n",
        "\"O gato perseguiu um inseto pequeno\",\n",
        "\"A porta abriu com dificuldade\",\n",
        "\"O menino guardou o carrinho azul\",\n",
        "\"A cozinheira temperou a carne fresca\",\n",
        "\"O rep√≥rter escreveu uma mat√©ria r√°pida\",\n",
        "\"A cabra atravessou a ponte velha\",\n",
        "\"O jogador marcou um ponto importante\",\n",
        "\"A bailarina sorriu para o p√∫blico\",\n",
        "\"O cantineiro serviu a refei√ß√£o quente\",\n",
        "\"A √°gua escorreu pela calha met√°lica\",\n",
        "\"O p√°ssaro cantou no galho fino\",\n",
        "\"A enfermeira aplicou uma inje√ß√£o leve\",\n",
        "\"O garoto montou um quebra-cabe√ßa dif√≠cil\",\n",
        "\"A sombra cobriu parte do ch√£o\",\n",
        "\"O trem diminuiu a velocidade\",\n",
        "\"A menina segurou o guarda-chuva azul\",\n",
        "\"O gato explorou a casa inteira\",\n",
        "\"A m√£e lavou a lou√ßa rapidamente\",\n",
        "\"O cachorro latiu para a porta\",\n",
        "\"A crian√ßa pulou sobre a po√ßa\",\n",
        "\"O livro caiu da estante alta\",\n",
        "\"O avi√£o pousou com suavidade\",\n",
        "\"A estrela piscou levemente no c√©u\",\n",
        "\"O gato dormiu no tapete macio\",\n",
        "\"A menina penteou os cabelos longos\",\n",
        "\"O atleta saltou sobre o obst√°culo\",\n",
        "\"A abelha voou at√© outra flor\",\n",
        "\"O menino jogou a bola longe\",\n",
        "\"A brisa refrescou a manh√£ clara\",\n",
        "\"O sapo pulou no lago raso\",\n",
        "\"A enfermeira sorriu para o paciente\",\n",
        "\"O pintor escolheu uma cor vibrante\",\n",
        "\"A menina cantou uma can√ß√£o doce\",\n",
        "\"O gato se escondeu atr√°s da porta\",\n",
        "\"A chuva caiu sobre o telhado velho\",\n",
        "\"O pescador recolheu a rede pesada\",\n",
        "\"A mo√ßa arrumou a mesa do jantar\",\n",
        "\"O c√£o encontrou um brinquedo velho\",\n",
        "\"A estrela iluminou o caminho curto\",\n",
        "\"O motorista estacionou o carro preto\",\n",
        "\"A crian√ßa desenhou um sol amarelo\",\n",
        "\"O vento virou a p√°gina do livro\",\n",
        "\"A garota colocou flores no vaso\",\n",
        "\"O homem apagou a luz da sala\",\n",
        "\"A bailarina girou com leveza\",\n",
        "\"O gato observou as pessoas na rua\",\n",
        "\"A menina abriu um presente rosa\",\n",
        "\"O barco balan√ßou nas ondas calmas\",\n",
        "\"O menino correu para o port√£o\",\n",
        "\"A professora escreveu no quadro branco\",\n",
        "\"O estudante revisou o texto final\",\n",
        "\"A mar√© subiu lentamente\",\n",
        "\"O m√∫sico tocou um acorde suave\",\n",
        "\"A crian√ßa segurou um brinquedo novo\",\n",
        "\"O carro acelerou na avenida larga\",\n",
        "\"A nuvem trouxe um pouco de sombra\",\n",
        "\"O gato saltou sobre a mesa\",\n",
        "\"A menina coloriu a folha inteira\",\n",
        "\"O homem empurrou a porta pesada\",\n",
        "\"A mulher acendeu uma vela pequena\",\n",
        "\"O pescador encontrou um peixe grande\",\n",
        "\"A menina soprou as velas do bolo\",\n",
        "\"O rel√≥gio despertou bem cedo\",\n",
        "\"A estrela caiu no horizonte\",\n",
        "\"O c√£o buscou a bolinha verde\",\n",
        "\"A crian√ßa montou uma torre alta\",\n",
        "\"O menino derrubou a caixa vazia\",\n",
        "\"A m√£e secou os pratos limpos\",\n",
        "\"O gato arranhou a porta de madeira\",\n",
        "\"A menina escreveu no caderno novo\",\n",
        "\"O m√∫sico ajustou o microfone alto\",\n",
        "\"A √°rvore balan√ßou com a ventania\",\n",
        "\"O amigo segurou a mochila pesada\",\n",
        "\"A crian√ßa observou o c√©u azul\",\n",
        "\"O homem procurou as chaves perdidas\",\n",
        "\"A borboleta voou sobre o jardim\",\n",
        "\"O cachorro abanou o rabo feliz\",\n",
        "\"A garota prendeu o cabelo curto\",\n",
        "\"O menino encontrou um chap√©u velho\",\n",
        "\"A l√¢mpada iluminou o corredor longo\",\n",
        "\"O peixe saltou fora d'√°gua\",\n",
        "\"A menina segurou a caixa colorida\",\n",
        "\"O carro desviou do buraco fundo\",\n",
        "\"A m√£e abra√ßou o filho pequeno\",\n",
        "\"O gato bebeu leite morno\",\n",
        "\"A nuvem se moveu rapidamente\",\n",
        "\"O atleta praticou um novo movimento\",\n",
        "\"A crian√ßa abriu a porta devagar\",\n",
        "\"O m√∫sico tocou o piano antigo\",\n",
        "\"A borboleta descansou na pedra\",\n",
        "\"O vento espalhou as folhas secas\",\n",
        "\"A menina procurou o brinquedo perdido\",\n",
        "\"O homem iniciou uma conversa curta\",\n",
        "\"A √°gua escorreu pela parede fria\",\n",
        "\"O gato caminhou pelo corredor\",\n",
        "\"A mo√ßa encontrou uma carta velha\",\n",
        "\"O menino riscou o papel branco\",\n",
        "\"A menina segurou o ursinho marrom\",\n",
        "\"O carro virou √† direita rapidamente\",\n",
        "\"A abelha pousou na mesa\",\n",
        "\"O cachorro roeu o osso novo\",\n",
        "\"A estrela brilhou sobre o mar\",\n",
        "\"O menino vestiu a jaqueta azul\",\n",
        "\"A crian√ßa correu no parquinho\",\n",
        "\"O gato caiu do sof√° macio\",\n",
        "\"A m√£e preparou a mochila escolar\",\n",
        "\"O estudante apagou o quadro sujo\",\n",
        "\"A √°rvore cresceu na pra√ßa central\",\n",
        "\"O vento levantou poeira fina\",\n",
        "\"A menina tocou a campainha\",\n",
        "\"O homem abriu uma gaveta pesada\",\n",
        "\"A √°gua invadiu o corredor\",\n",
        "\"O cachorro lambeu o prato vazio\",\n",
        "\"O gato pulou para o colo da dona\",\n",
        "\"A brisa moveu o cabelo da menina\",\n",
        "\"O menino colocou o bon√© vermelho\",\n",
        "\"A crian√ßa chutou a bola amarela\",\n",
        "\"O carro entrou na garagem pequena\",\n",
        "\"A professora fechou o livro grosso\",\n",
        "\"O artista exp√¥s um quadro novo\",\n",
        "\"A menina encontrou um anel brilhante\",\n",
        "\"O p√°ssaro pousou no muro baixo\",\n",
        "\"A chuva apagou as marcas do ch√£o\",\n",
        "\"O gato ronronou para a dona\",\n",
        "\"A vela derreteu lentamente\",\n",
        "\"O menino escondeu a caixa pequena\",\n",
        "\"A m√£e ajeitou o len√ßol da cama\",\n",
        "\"O avi√£o sobrevoou o oceano azul\",\n",
        "\"A nuvem cobriu a montanha branca\",\n",
        "\"O gato arrastou o pano rasgado\",\n",
        "\"A crian√ßa abra√ßou o pai sorrindo\",\n",
        "\"O vento derrubou uma garrafa vazia\",\n",
        "\"A menina fechou o caderno rosa\",\n",
        "\"O cachorro entrou na casinha quente\",\n",
        "\"A luz refletiu no espelho limpo\",\n",
        "\"O homem encontrou uma carteira\",\n",
        "\"A crian√ßa soprou bolhas de sab√£o\",\n",
        "\"A mo√ßa escreveu uma mensagem r√°pida\"\n",
        "]\n",
        "\n",
        "file_path = \"corpus.txt\"\n",
        "\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in sentences:\n",
        "        f.write(sentence + \"\\n\")\n",
        "\n",
        "print(f\"File '{file_path}' created successfully with {len(sentences)} sentences.\")\n",
        "\n",
        "# Optionally, display the content to verify\n",
        "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     content = f.read()\n",
        "#     print(\"\\nContent of corpus.txt:\\n\", content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 - Pr√©-tokeniza√ß√£o\n"
      ],
      "metadata": {
        "id": "2ZRQpsLhIGNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers\n",
        "\n",
        "tok_ws = Tokenizer(models.BPE())\n",
        "tok_ws.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "frase = \"O menino voou no c√©u azul\"\n",
        "\n",
        "print(tok_ws.pre_tokenizer.pre_tokenize_str(frase))\n"
      ],
      "metadata": {
        "id": "HKHYbj5-FU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Punctuation + Whitespace*"
      ],
      "metadata": {
        "id": "XCj0m4asF14E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_punc = Tokenizer(models.BPE())\n",
        "tok_punc.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),\n",
        "    pre_tokenizers.Punctuation()\n",
        "])\n",
        "print(tok_punc.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "GEhKhK5UFoZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretokenizer: ByteLevel - estilo GPT-2**"
      ],
      "metadata": {
        "id": "TE4jFk36Gw2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_byte = Tokenizer(models.BPE())\n",
        "tok_byte.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "print(tok_byte.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "Q9AANXjUF8wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metaspace (SentencePiece style)"
      ],
      "metadata": {
        "id": "MrgFHmqvG3tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_meta = Tokenizer(models.BPE())\n",
        "tok_meta.pre_tokenizer = pre_tokenizers.Metaspace()\n",
        "print(tok_meta.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "h_5CuANPG55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento\n"
      ],
      "metadata": {
        "id": "XH9EDxlNH5PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 02_tokenizer_train.ipynb\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "# 1. Vis√£o geral do algoritmo BPE\n",
        "print(\"Treinar o tokenizador (BPE):\\n\")\n",
        "print(\"1. Comece com todos os caracteres presentes no corpus como tokens.\")\n",
        "print(\"2. Encontre e una o par de tokens mais frequente em um novo token.\")\n",
        "print(\"3. Repita at√© atingir o tamanho de vocabul√°rio desejado.\\n\")\n",
        "\n",
        "# 2. Corpus de treino\n",
        "file_path = \"corpus.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus = [line.strip() for line in f if line.strip()]\n",
        "print(\"Corpus de treino (primeiras 230 frases):\", corpus[:230], \"\\n\")\n",
        "\n",
        "# 3. Configura√ß√£o do tokenizador\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "bpe_trainer = trainers.BpeTrainer(\n",
        "    vocab_size=100,\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
        ")\n",
        "\n",
        "# 4. Treinamento\n",
        "tokenizer.train_from_iterator(corpus, trainer=bpe_trainer)\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(f\"Tamanho do vocabul√°rio: {len(vocab)}\\n\")\n",
        "\n",
        "# 5. Visualizando parte do vocabul√°rio\n",
        "sorted_vocab = sorted(vocab.items(), key=lambda kv: kv[1])[:20]\n",
        "for token, idx in sorted_vocab:\n",
        "    print(f\"{idx:>3} \\u2192 {repr(token)}\")\n",
        "\n",
        "# 6. Salvando e recarregando\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "tokenizer_new = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# 7. Testando em novas frases\n",
        "textos = [\"O rato roeu a roupa do rei de Roma\", \"Hello, world.\"]\n",
        "\n",
        "print(\"\\nTokeniza√ß√£o de exemplos:\")\n",
        "for texto in textos:\n",
        "    out = tokenizer_new.encode(texto)\n",
        "    print(f\"Texto: {texto}\")\n",
        "    print(f\"Tokens: {out.tokens}\")\n",
        "    print(f\"IDs: {out.ids}\\n\")"
      ],
      "metadata": {
        "id": "3KIL27TjILDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode"
      ],
      "metadata": {
        "id": "3bkj5aXXJ3xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_tokenizer_encode.ipynb\n",
        "# Pipeline de tokeniza√ß√£o: normaliza√ß√£o ‚Üí pr√©-tokeniza√ß√£o ‚Üí modelo ‚Üí p√≥s-processamento\n",
        "\n",
        "from tokenizers import Tokenizer, normalizers, pre_tokenizers, processors\n",
        "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace, Digits, Sequence\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "print(\"### Pipeline de tokeniza√ß√£o ###\")\n",
        "print(\" Normalization\")\n",
        "print(\" Pre-tokenization\")\n",
        "print(\" Model\")\n",
        "print(\" Post-processing\\n\")\n",
        "\n",
        "# Carregar o tokenizador treinado (BPE)\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Normalization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Normalization\")\n",
        "normalizer = normalizers.Sequence([\n",
        "    NFD(),          # decomposi√ß√£o de acentos\n",
        "    Lowercase(),    # tudo min√∫sculo\n",
        "    StripAccents()  # remove acentos\n",
        "])\n",
        "texto = \"H√©ll√≤ h√¥w are √º?\"\n",
        "print(\"Antes:\", texto)\n",
        "print(\"Depois:\", normalizer.normalize_str(texto), \"\\n\")\n",
        "tokenizer.normalizer = normalizer\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Pre-tokenization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Pre-tokenization\")\n",
        "pre_tok = Sequence([\n",
        "    Whitespace(),\n",
        "    Digits(individual_digits=True)\n",
        "])\n",
        "texto2 = \"Hello! How are you? Tenho R$ 213,12.\"\n",
        "print(\"Pr√©-tokeniza√ß√£o:\", pre_tok.pre_tokenize_str(texto2), \"\\n\")\n",
        "tokenizer.pre_tokenizer = pre_tok\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Model\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Model: BPE (Byte Pair Encoding)\")\n",
        "# j√° carregado do arquivo bpe_tokenizer.json\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Post-processing\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Post-processing (TemplateProcessing)\")\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Aplicando tudo\n",
        "# -----------------------------------------------------------\n",
        "encoded = tokenizer.encode(\"ol√° mundo\")\n",
        "print(\"Tokens IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)\n"
      ],
      "metadata": {
        "id": "WLRBw0K_J1_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bytelevel vs SentencePiece"
      ],
      "metadata": {
        "id": "_XKY1lw2NuNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_bytelevel_vs_sentencepiece.ipynb\n",
        "# Comparando ByteLevel (GPT-2) vs SentencePiece (mT5)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import unicodedata\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Modelos\n",
        "# -----------------------------\n",
        "BYTELEVEL_MODEL = \"openai-community/gpt2\"\n",
        "SENTPIECE_MODEL = \"google/mt5-small\"\n",
        "\n",
        "tok_byte = AutoTokenizer.from_pretrained(BYTELEVEL_MODEL)\n",
        "tok_spm  = AutoTokenizer.from_pretrained(SENTPIECE_MODEL)\n",
        "\n",
        "# Garantir pad_token\n",
        "if tok_byte.pad_token is None and hasattr(tok_byte, \"eos_token\"):\n",
        "    tok_byte.pad_token = tok_byte.eos_token\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Texto de exemplo\n",
        "# -----------------------------\n",
        "text = \"Vamos comer, vov√≥! üôÇ\"\n",
        "print(f\"Texto: {text}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Tokeniza√ß√£o\n",
        "# -----------------------------\n",
        "def encode_details(tokenizer, name):\n",
        "    enc = tokenizer(text, add_special_tokens=True, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    ids = enc[\"input_ids\"]\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"IDs:\", ids)\n",
        "    print(\"Qtd tokens:\", len(tokens))\n",
        "    print(\"Decoded:\", tokenizer.decode(ids))\n",
        "    print(\"Offsets:\", offsets)\n",
        "    print()\n",
        "\n",
        "encode_details(tok_byte, \"ByteLevel (GPT-2)\")\n",
        "encode_details(tok_spm, \"SentencePiece (mT5)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Compara√ß√£o Unicode (opcional)\n",
        "# -----------------------------\n",
        "def show_unicode_chars(s):\n",
        "    for ch in s:\n",
        "        name = unicodedata.name(ch, \"UNKNOWN\")\n",
        "        print(f\"{repr(ch)} -> {name}\")\n",
        "\n",
        "print(\"\\nCaracteres Unicode do texto:\")\n",
        "show_unicode_chars(text)\n"
      ],
      "metadata": {
        "id": "mhEG4EpoLiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avalia√ß√£o"
      ],
      "metadata": {
        "id": "cj23lfrwPLjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Carrega o tokenizador treinado\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# Corpus de teste (pode ser parte do seu corpus real)\n",
        "test_texts = [\n",
        "    \"O rato roeu a roupa do rei de Roma.\",\n",
        "    \"Aprender tokeniza√ß√£o √© divertido!\",\n",
        "    \"GPT-2 e mT5 usam abordagens diferentes.\",\n",
        "    \"Python √© √≥timo para NLP üòÑ\",\n",
        "]\n",
        "\n",
        "# Fun√ß√µes auxiliares\n",
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def evaluate_tokenizer(tokenizer, texts):\n",
        "    stats = []\n",
        "    for t in texts:\n",
        "        enc = tokenizer.encode(t)\n",
        "        stats.append({\n",
        "            \"text\": t,\n",
        "            \"chars\": count_chars(t),\n",
        "            \"words\": count_words(t),\n",
        "            \"tokens\": len(enc.tokens),\n",
        "            \"unk\": enc.tokens.count(\"<unk>\"),\n",
        "            \"decoded_ok\": (tokenizer.decode(enc.ids) == t)\n",
        "        })\n",
        "    return stats\n",
        "\n",
        "stats = evaluate_tokenizer(tokenizer, test_texts)\n",
        "\n",
        "# Converter para m√©tricas agregadas\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(stats)\n",
        "\n",
        "tpc = (df[\"tokens\"] / df[\"chars\"]).mean()\n",
        "tpw = (df[\"tokens\"] / df[\"words\"]).mean()\n",
        "unk_rate = (df[\"unk\"].sum() / df[\"tokens\"].sum()) * 100\n",
        "decode_acc = (df[\"decoded_ok\"].mean()) * 100\n",
        "\n",
        "print(\"=== M√©tricas de efici√™ncia ===\")\n",
        "print(f\"Tokens por caractere (TPC): {tpc:.3f}\")\n",
        "print(f\"Tokens por palavra (TPW): {tpw:.3f}\")\n",
        "print(f\"Percentual de <unk>: {unk_rate:.2f}%\")\n",
        "print(f\"Reversibilidade (decode == original): {decode_acc:.1f}%\")\n",
        "print(f\"Tamanho m√©dio da sequ√™ncia: {df['tokens'].mean():.1f} tokens/frase\")\n"
      ],
      "metadata": {
        "id": "hfiIxoW5PN6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}