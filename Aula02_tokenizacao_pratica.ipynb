{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroTonus/praticasGSI073/blob/main/Aula02_tokenizacao_pratica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 2 - Tokeniza√ß√£o"
      ],
      "metadata": {
        "id": "R8ElgxbDEATe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "846092fa"
      },
      "source": [
        "sentences = [\n",
        "\"Imagine there's no heaven\",\n",
        "\"We don't need no education\",\n",
        "\"It's better to burn out than to fade away\",\n",
        "\"I want to hold your hand\",\n",
        "\"Don't stop believin'\",\n",
        "\"Born to be wild\",\n",
        "\"Is this the real life?\",\n",
        "\"Hey Jude, don't make it bad\",\n",
        "\"I can't get no satisfaction\",\n",
        "\"Every little thing she does is magic\",\n",
        "\"You can't always get what you want\",\n",
        "\"It's a beautiful day\",\n",
        "\"Smells like teen spirit\",\n",
        "\"I want to break free\",\n",
        "\"Sweet child o' mine\",\n",
        "\"With the lights out, it's less dangerous\",\n",
        "\"We are the champions\",\n",
        "\"Under pressure\",\n",
        "\"I see a red door and I want it painted black\",\n",
        "\"You were made for loving me\",\n",
        "\"Knockin' on heaven's door\",\n",
        "\"I am the walrus\",\n",
        "\"Give peace a chance\",\n",
        "\"I'm a joker, I'm a smoker\",\n",
        "\"I shot the sheriff\",\n",
        "\"No woman, no cry\",\n",
        "\"You shook me all night long\",\n",
        "\"Take a walk on the wild side\",\n",
        "\"Another brick in the wall\",\n",
        "\"Come as you are\",\n",
        "\"Say it ain't so\",\n",
        "\"Don't fear the reaper\",\n",
        "\"I believe in a thing called love\",\n",
        "\"Livin' on a prayer\",\n",
        "\"Paradise city\",\n",
        "\"Hello, I love you\",\n",
        "\"One, two, three, four\",\n",
        "\"Let it be\",\n",
        "\"Here comes the sun\",\n",
        "\"All you need is love\",\n",
        "\"Back in black\",\n",
        "\"Highway to hell\",\n",
        "\"Thunderstruck\",\n",
        "\"Whole lotta love\",\n",
        "\"Kashmir\",\n",
        "\"Black dog\",\n",
        "\"Light my fire\",\n",
        "\"Love me two times\",\n",
        "\"Break on through\",\n",
        "\"Born in the U.S.A.\",\n",
        "\"Dancing in the dark\",\n",
        "\"Like a rolling stone\",\n",
        "\"Blowin' in the wind\",\n",
        "\"Hotel California\",\n",
        "\"Life in the fast lane\",\n",
        "\"Wish you were here\",\n",
        "\"Time is on my side\",\n",
        "\"Start me up\",\n",
        "\"Heroes\",\n",
        "\"Rebel rebel\",\n",
        "\"Ziggy played guitar\",\n",
        "\"All right now\",\n",
        "\"Smoke on the water\",\n",
        "\"Carry on my wayward son\",\n",
        "\"More than a feeling\",\n",
        "\"Dream on\",\n",
        "\"Walk this way\",\n",
        "\"Sweet emotion\",\n",
        "\"Don't look back in anger\",\n",
        "\"Wonderwall\",\n",
        "\"Champagne supernova\",\n",
        "\"London calling\",\n",
        "\"Should I stay or should I go\",\n",
        "\"I fought the law\",\n",
        "\"Baba O'Riley\",\n",
        "\"Pinball wizard\",\n",
        "\"My generation\",\n",
        "\"Sweet home Alabama\",\n",
        "\"Free bird\",\n",
        "\"American woman\",\n",
        "\"Sharp dressed man\",\n",
        "\"In the end\",\n",
        "\"Numb\",\n",
        "\"Californication\",\n",
        "\"Under the bridge\",\n",
        "\"By the way\",\n",
        "\"Seven nation army\",\n",
        "\"Fell in love with a girl\",\n",
        "\"Mr. Brightside\",\n",
        "\"Somebody told me\",\n",
        "\"Welcome to the jungle\",\n",
        "\"Nightrain\",\n",
        "\"Walk on\",\n",
        "\"Sunday bloody Sunday\",\n",
        "\"Everlong\",\n",
        "\"Learn to fly\",\n",
        "\"Black hole sun\",\n",
        "\"Spoonman\"\n",
        "]\n",
        "\n",
        "file_path = \"corpus.txt\"\n",
        "\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in sentences:\n",
        "        f.write(sentence + \"\\n\")\n",
        "\n",
        "print(f\"File '{file_path}' created successfully with {len(sentences)} sentences.\")\n",
        "\n",
        "# Optionally, display the content to verify\n",
        "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     content = f.read()\n",
        "#     print(\"\\nContent of corpus.txt:\\n\", content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 - Pr√©-tokeniza√ß√£o\n"
      ],
      "metadata": {
        "id": "2ZRQpsLhIGNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers\n",
        "\n",
        "tok_ws = Tokenizer(models.BPE())\n",
        "tok_ws.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "frase = \"Come as you are.\"\n",
        "\n",
        "print(tok_ws.pre_tokenizer.pre_tokenize_str(frase))\n"
      ],
      "metadata": {
        "id": "HKHYbj5-FU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Punctuation + Whitespace*"
      ],
      "metadata": {
        "id": "XCj0m4asF14E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_punc = Tokenizer(models.BPE())\n",
        "tok_punc.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),\n",
        "    pre_tokenizers.Punctuation()\n",
        "])\n",
        "print(tok_punc.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "GEhKhK5UFoZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretokenizer: ByteLevel - estilo GPT-2**"
      ],
      "metadata": {
        "id": "TE4jFk36Gw2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_byte = Tokenizer(models.BPE())\n",
        "tok_byte.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "print(tok_byte.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "Q9AANXjUF8wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metaspace (SentencePiece style)"
      ],
      "metadata": {
        "id": "MrgFHmqvG3tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_meta = Tokenizer(models.BPE())\n",
        "tok_meta.pre_tokenizer = pre_tokenizers.Metaspace()\n",
        "print(tok_meta.pre_tokenizer.pre_tokenize_str(frase))"
      ],
      "metadata": {
        "id": "h_5CuANPG55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento\n"
      ],
      "metadata": {
        "id": "XH9EDxlNH5PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 02_tokenizer_train.ipynb\n",
        "\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "# 1. Vis√£o geral do algoritmo BPE\n",
        "print(\"Treinar o tokenizador (BPE):\\n\")\n",
        "print(\"1. Comece com todos os caracteres presentes no corpus como tokens.\")\n",
        "print(\"2. Encontre e una o par de tokens mais frequente em um novo token.\")\n",
        "print(\"3. Repita at√© atingir o tamanho de vocabul√°rio desejado.\\n\")\n",
        "\n",
        "# 2. Corpus de treino\n",
        "file_path = \"corpus.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus = [line.strip() for line in f if line.strip()]\n",
        "print(\"Corpus de treino (primeiras 100 frases):\", corpus[:100], \"\\n\")\n",
        "\n",
        "# 3. Configura√ß√£o do tokenizador\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "bpe_trainer = trainers.BpeTrainer(\n",
        "    vocab_size=50,\n",
        "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
        ")\n",
        "\n",
        "# 4. Treinamento\n",
        "tokenizer.train_from_iterator(corpus, trainer=bpe_trainer)\n",
        "vocab = tokenizer.get_vocab()\n",
        "print(f\"Tamanho do vocabul√°rio: {len(vocab)}\\n\")\n",
        "\n",
        "# 5. Visualizando parte do vocabul√°rio\n",
        "sorted_vocab = sorted(vocab.items(), key=lambda kv: kv[1])[:20]\n",
        "for token, idx in sorted_vocab:\n",
        "    print(f\"{idx:>3} \\u2192 {repr(token)}\")\n",
        "\n",
        "# 6. Salvando e recarregando\n",
        "tokenizer.save(\"bpe_tokenizer.json\")\n",
        "tokenizer_new = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# 7. Testando em novas frases\n",
        "textos = [\"You shook me all night long\", \"Hello, world.\"]\n",
        "\n",
        "print(\"\\nTokeniza√ß√£o de exemplos:\")\n",
        "for texto in textos:\n",
        "    out = tokenizer_new.encode(texto)\n",
        "    print(f\"Texto: {texto}\")\n",
        "    print(f\"Tokens: {out.tokens}\")\n",
        "    print(f\"IDs: {out.ids}\\n\")"
      ],
      "metadata": {
        "id": "3KIL27TjILDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode"
      ],
      "metadata": {
        "id": "3bkj5aXXJ3xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_tokenizer_encode.ipynb\n",
        "# Pipeline de tokeniza√ß√£o: normaliza√ß√£o ‚Üí pr√©-tokeniza√ß√£o ‚Üí modelo ‚Üí p√≥s-processamento\n",
        "\n",
        "from tokenizers import Tokenizer, normalizers, pre_tokenizers, processors\n",
        "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace, Digits, Sequence\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "print(\"### Pipeline de tokeniza√ß√£o ###\")\n",
        "print(\" Normalization\")\n",
        "print(\" Pre-tokenization\")\n",
        "print(\" Model\")\n",
        "print(\" Post-processing\\n\")\n",
        "\n",
        "# Carregar o tokenizador treinado (BPE)\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Normalization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Normalization\")\n",
        "normalizer = normalizers.Sequence([\n",
        "    NFD(),          # decomposi√ß√£o de acentos\n",
        "    Lowercase(),    # tudo min√∫sculo\n",
        "    StripAccents()  # remove acentos\n",
        "])\n",
        "texto = \"H√©ll√≤ h√¥w are √º?\"\n",
        "print(\"Antes:\", texto)\n",
        "print(\"Depois:\", normalizer.normalize_str(texto), \"\\n\")\n",
        "tokenizer.normalizer = normalizer\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Pre-tokenization\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Pre-tokenization\")\n",
        "pre_tok = Sequence([\n",
        "    Whitespace(),\n",
        "    Digits(individual_digits=True)\n",
        "])\n",
        "texto2 = \"Hello! How are you? Tenho R$ 213,12.\"\n",
        "print(\"Pr√©-tokeniza√ß√£o:\", pre_tok.pre_tokenize_str(texto2), \"\\n\")\n",
        "tokenizer.pre_tokenizer = pre_tok\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Model\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Model: BPE (Byte Pair Encoding)\")\n",
        "# j√° carregado do arquivo bpe_tokenizer.json\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Post-processing\n",
        "# -----------------------------------------------------------\n",
        "print(\"# Post-processing (TemplateProcessing)\")\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
        ")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Aplicando tudo\n",
        "# -----------------------------------------------------------\n",
        "encoded = tokenizer.encode(\"ol√° mundo\")\n",
        "print(\"Tokens IDs:\", encoded.ids)\n",
        "print(\"Tokens:\", encoded.tokens)\n"
      ],
      "metadata": {
        "id": "WLRBw0K_J1_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bytelevel vs SentencePiece"
      ],
      "metadata": {
        "id": "_XKY1lw2NuNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_bytelevel_vs_sentencepiece.ipynb\n",
        "# Comparando ByteLevel (GPT-2) vs SentencePiece (mT5)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import unicodedata\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Modelos\n",
        "# -----------------------------\n",
        "BYTELEVEL_MODEL = \"openai-community/gpt2\"\n",
        "SENTPIECE_MODEL = \"google/mt5-small\"\n",
        "\n",
        "tok_byte = AutoTokenizer.from_pretrained(BYTELEVEL_MODEL)\n",
        "tok_spm  = AutoTokenizer.from_pretrained(SENTPIECE_MODEL)\n",
        "\n",
        "# Garantir pad_token\n",
        "if tok_byte.pad_token is None and hasattr(tok_byte, \"eos_token\"):\n",
        "    tok_byte.pad_token = tok_byte.eos_token\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Texto de exemplo\n",
        "# -----------------------------\n",
        "text = \"Vamos comer, vov√≥! üôÇ\"\n",
        "print(f\"Texto: {text}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Tokeniza√ß√£o\n",
        "# -----------------------------\n",
        "def encode_details(tokenizer, name):\n",
        "    enc = tokenizer(text, add_special_tokens=True, return_offsets_mapping=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
        "    ids = enc[\"input_ids\"]\n",
        "    offsets = enc[\"offset_mapping\"]\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"IDs:\", ids)\n",
        "    print(\"Qtd tokens:\", len(tokens))\n",
        "    print(\"Decoded:\", tokenizer.decode(ids))\n",
        "    print(\"Offsets:\", offsets)\n",
        "    print()\n",
        "\n",
        "encode_details(tok_byte, \"ByteLevel (GPT-2)\")\n",
        "encode_details(tok_spm, \"SentencePiece (mT5)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Compara√ß√£o Unicode (opcional)\n",
        "# -----------------------------\n",
        "def show_unicode_chars(s):\n",
        "    for ch in s:\n",
        "        name = unicodedata.name(ch, \"UNKNOWN\")\n",
        "        print(f\"{repr(ch)} -> {name}\")\n",
        "\n",
        "print(\"\\nCaracteres Unicode do texto:\")\n",
        "show_unicode_chars(text)\n"
      ],
      "metadata": {
        "id": "mhEG4EpoLiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avalia√ß√£o"
      ],
      "metadata": {
        "id": "cj23lfrwPLjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Carrega o tokenizador treinado\n",
        "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
        "\n",
        "# Corpus de teste (pode ser parte do seu corpus real)\n",
        "test_texts = [\n",
        "    \"O rato roeu a roupa do rei de Roma.\",\n",
        "    \"Aprender tokeniza√ß√£o √© divertido!\",\n",
        "    \"GPT-2 e mT5 usam abordagens diferentes.\",\n",
        "    \"Python √© √≥timo para NLP üòÑ\",\n",
        "]\n",
        "\n",
        "# Fun√ß√µes auxiliares\n",
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def evaluate_tokenizer(tokenizer, texts):\n",
        "    stats = []\n",
        "    for t in texts:\n",
        "        enc = tokenizer.encode(t)\n",
        "        stats.append({\n",
        "            \"text\": t,\n",
        "            \"chars\": count_chars(t),\n",
        "            \"words\": count_words(t),\n",
        "            \"tokens\": len(enc.tokens),\n",
        "            \"unk\": enc.tokens.count(\"<unk>\"),\n",
        "            \"decoded_ok\": (tokenizer.decode(enc.ids) == t)\n",
        "        })\n",
        "    return stats\n",
        "\n",
        "stats = evaluate_tokenizer(tokenizer, test_texts)\n",
        "\n",
        "# Converter para m√©tricas agregadas\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(stats)\n",
        "\n",
        "tpc = (df[\"tokens\"] / df[\"chars\"]).mean()\n",
        "tpw = (df[\"tokens\"] / df[\"words\"]).mean()\n",
        "unk_rate = (df[\"unk\"].sum() / df[\"tokens\"].sum()) * 100\n",
        "decode_acc = (df[\"decoded_ok\"].mean()) * 100\n",
        "\n",
        "print(\"=== M√©tricas de efici√™ncia ===\")\n",
        "print(f\"Tokens por caractere (TPC): {tpc:.3f}\")\n",
        "print(f\"Tokens por palavra (TPW): {tpw:.3f}\")\n",
        "print(f\"Percentual de <unk>: {unk_rate:.2f}%\")\n",
        "print(f\"Reversibilidade (decode == original): {decode_acc:.1f}%\")\n",
        "print(f\"Tamanho m√©dio da sequ√™ncia: {df['tokens'].mean():.1f} tokens/frase\")\n"
      ],
      "metadata": {
        "id": "hfiIxoW5PN6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}