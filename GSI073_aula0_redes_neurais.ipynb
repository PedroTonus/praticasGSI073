{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroTonus/praticasGSI073/blob/main/GSI073_aula0_redes_neurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GSI073 - Tópicos Especiais de Inteligência Artificial\n",
        "\n",
        "## Definição dos dados"
      ],
      "metadata": {
        "id": "Mwsc0ViVertv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6782a1fc"
      },
      "source": [
        "## Avaliação do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmZxMYLGefOh"
      },
      "outputs": [],
      "source": [
        "import torch; import sklearn; from torch import nn\n",
        "\n",
        "# 1. Carregar dados\n",
        "iris = sklearn.datasets.load_iris()\n",
        "X = iris.data        # 4 features: sépalas e pétalas\n",
        "y = (iris.target == 1).astype(float)  # 1 se Versicolor, 0 caso contrário\n",
        "\n",
        "# 2. Preparar dados para pytorch\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32).view(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do modelo e treinamento"
      ],
      "metadata": {
        "id": "nUv-LKlIe9Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definir modelo\n",
        "import torch.nn.functional as F\n",
        "class RedeNeural(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(RedeNeural, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eg97DxIbe0tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar modelo\n",
        "modelo = RedeNeural(4, 8, 1)  # 4 features → 1 saída (probabilidade de ser Versicolor)\n",
        "\n",
        "import copy\n",
        "modelo_clonado = copy.deepcopy(modelo)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Definir função de perda e algoritmo de otimização\n",
        "funcao_perda = torch.nn.BCEWithLogitsLoss()  # combinação de sigmoid + BCE\n",
        "optimizer = torch.optim.SGD(modelo.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "muU3pzeUFhHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execução do treinamento com optimizer SGD"
      ],
      "metadata": {
        "id": "Agjn3aQxfHOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop de treino\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()           # Limpa gradientes\n",
        "    outputs = modelo(X)             # Forward\n",
        "    loss = funcao_perda(outputs, y) # Calcula perda\n",
        "    loss.backward()                 # Calcula derivadas do gradiente\n",
        "    optimizer.step()                # Aplica regra de alteração dos parâmetros\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Época [{epoch+1}/100], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "uuksjyq7e4Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treino com regra de gradiente descendente manual"
      ],
      "metadata": {
        "id": "SVolkVjiNEa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()           # Limpa gradientes\n",
        "    outputs = modelo_clonado(X)     # Forward\n",
        "    loss = funcao_perda(outputs, y) # Calcula perda\n",
        "    loss.backward()                 # Calcula derivadas do gradiente\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in modelo_clonado.parameters():\n",
        "            param -= learning_rate * param.grad  # regra de atualização de pesos\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Época [{epoch+1}/1000], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "CW9eAqNJMRD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad321ea1"
      },
      "source": [
        "# Fazer previsões no conjunto de treinamento\n",
        "with torch.no_grad():\n",
        "    outputs = modelo(X)\n",
        "    predicted_probs = torch.sigmoid(outputs) # Converter logits para probabilidades\n",
        "    predicted_classes = (predicted_probs >= 0.5).float() # Classificar 0 ou 1\n",
        "\n",
        "# Calcular acurácia\n",
        "accuracy = (predicted_classes == y).float().mean()\n",
        "print(f\"Acurácia do modelo no conjunto de treinamento: {accuracy.item():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19eeab7f"
      },
      "source": [
        "# Task\n",
        "Recuperate the loss value at epoch 100 for the model with `hidden_dim=8`. Then, train a new model with `hidden_dim=16` and retrieve its loss value at epoch 100. Finally, compare these two loss values and explain, in Portuguese, the impact of increasing the number of neurons in the hidden layer on the loss at epoch 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4948ca"
      },
      "source": [
        "## Recuperar Loss Original\n",
        "\n",
        "### Subtask:\n",
        "Recuperar o valor da loss na época 100 para o modelo com `hidden_dim=8`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "792fbec6"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires retrieving the loss value at epoch 100 from the training output of the model with `hidden_dim=8`. I have identified this value from the provided execution output. Now I will store this value in a variable as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d28b54f"
      },
      "source": [
        "loss_epoch_100_hd8 = 0.0495\n",
        "print(f\"Loss na Época 100 para o modelo com hidden_dim=8: {loss_epoch_100_hd8}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbf44c0f"
      },
      "source": [
        "### Subtask:\n",
        "Treinar um novo modelo com `hidden_dim=16` e recuperar o valor da loss na época 100.\n",
        "\n",
        "#### Instructions\n",
        "1. Crie uma nova instância da classe `RedeNeural` com `hidden_dim=16` e `input_dim=4`, `output_dim=1`. Chame este novo modelo de `modelo_hd16`.\n",
        "2. Defina uma nova função de perda (`funcao_perda_hd16`) e um novo otimizador (`optimizer_hd16`) para `modelo_hd16`, usando `BCEWithLogitsLoss` e `SGD` com `learning_rate=0.1`.\n",
        "3. Treine `modelo_hd16` por 1000 épocas, seguindo o mesmo loop de treinamento do `modelo` original.\n",
        "4. Localize a saída de execução para `modelo_hd16` e observe o valor da perda (Loss) reportado para a época 100.\n",
        "5. Armazene este valor em uma variável para uso posterior, como `loss_epoch_100_hd16`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c42cf86"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous markdown block outlined the steps to train a new model with `hidden_dim=16`. Now, I will implement the first two instructions: create the new model instance (`modelo_hd16`), define its loss function (`funcao_perda_hd16`), and its optimizer (`optimizer_hd16`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67454219"
      },
      "source": [
        "modelo_hd16 = RedeNeural(4, 16, 1) # 4 features -> 1 saída (probabilidade de ser Versicolor)\n",
        "funcao_perda_hd16 = torch.nn.BCEWithLogitsLoss() # combinação de sigmoid + BCE\n",
        "optimizer_hd16 = torch.optim.SGD(modelo_hd16.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"Novo modelo com hidden_dim=16, função de perda e otimizador definidos.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3041c619"
      },
      "source": [
        "**Reasoning**:\n",
        "The new model (`modelo_hd16`) with `hidden_dim=16` has been defined along with its loss function and optimizer. The next step is to train this model for 1000 epochs, as specified in the instructions. I will include a print statement to display the loss every 100 epochs, similar to the previous training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e47629f4"
      },
      "source": [
        "for epoch in range(1000):\n",
        "    optimizer_hd16.zero_grad()           # Limpa gradientes\n",
        "    outputs_hd16 = modelo_hd16(X)         # Forward\n",
        "    loss_hd16 = funcao_perda_hd16(outputs_hd16, y) # Calcula perda\n",
        "    loss_hd16.backward()                 # Calcula derivadas do gradiente\n",
        "    optimizer_hd16.step()                # Aplica regra de alteração dos parâmetros\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Época [{epoch+1}/1000], Loss: {loss_hd16.item():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47ce20f"
      },
      "source": [
        "**Reasoning**:\n",
        "The training of `modelo_hd16` is complete, and the loss at epoch 100 is visible in the execution output. I will now extract this value and store it in the `loss_epoch_100_hd16` variable as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38bd86e3"
      },
      "source": [
        "loss_epoch_100_hd16 = 0.4835\n",
        "print(f\"Loss na Época 100 para o modelo com hidden_dim=16: {loss_epoch_100_hd16}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8dab0a"
      },
      "source": [
        "### Comparação e Explicação\n",
        "\n",
        "Agora que temos os valores de perda para ambos os modelos na época 100, podemos compará-los e analisar o impacto do aumento do número de neurônios na camada oculta.\n",
        "\n",
        "- **Loss na Época 100 para `hidden_dim=8`**: {{loss_epoch_100_hd8}}\n",
        "- **Loss na Época 100 para `hidden_dim=16`**: {{loss_epoch_100_hd16}}\n",
        "\n",
        "**Impacto do aumento de neurônios na camada oculta:**\n",
        "\n",
        "Ao comparar os valores de perda, observamos que o modelo com `hidden_dim=8` (Loss: {{loss_epoch_100_hd8}}) apresentou uma perda significativamente menor na época 100 do que o modelo com `hidden_dim=16` (Loss: {{loss_epoch_100_hd16}}).\n",
        "\n",
        "Um aumento no número de neurônios na camada oculta (`hidden_dim`) geralmente confere ao modelo uma maior capacidade de representação e complexidade. No entanto, no início do treinamento (como na época 100), um modelo mais complexo pode ter uma performance pior por algumas razões:\n",
        "\n",
        "1.  **Inicialização aleatória**: Com mais parâmetros, há uma chance maior de que a inicialização aleatória dos pesos coloque o modelo em um ponto desfavorável no espaço de busca de pesos.\n",
        "2.  **Velocidade de convergência**: Modelos mais complexos podem exigir mais épocas para convergir, ou seja, para que seus pesos se ajustem adequadamente e a perda comece a diminuir consistentemente. A capacidade extra pode levar mais tempo para ser aproveitada.\n",
        "3.  **Overfitting inicial**: Embora o overfitting seja mais comum em estágios posteriores do treinamento, um modelo muito complexo pode começar a se ajustar ao ruído nos dados de treinamento mais rapidamente, impedindo que a perda geral diminua de forma eficiente nas primeiras épocas, especialmente se os dados de treinamento forem limitados ou ruidosos.\n",
        "\n",
        "Neste caso específico, na época 100, o modelo mais simples (`hidden_dim=8`) parece ter se estabilizado mais rapidamente ou encontrado um vale mais raso na função de perda, resultando em um valor de perda menor. Isso sugere que, para a tarefa e os dados em questão, uma complexidade moderada já é suficiente ou que o modelo mais complexo ainda está em uma fase inicial de adaptação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e36a4e58"
      },
      "source": [
        "### Comparação e Explicação\n",
        "\n",
        "Agora que temos os valores de perda para ambos os modelos na época 100, podemos compará-los e analisar o impacto do aumento do número de neurônios na camada oculta.\n",
        "\n",
        "- **Loss na Época 100 para `hidden_dim=8`**: {{loss_epoch_100_hd8}}\n",
        "- **Loss na Época 100 para `hidden_dim=16`**: {{loss_epoch_100_hd16}}\n",
        "\n",
        "**Impacto do aumento de neurônios na camada oculta:**\n",
        "\n",
        "Ao comparar os valores de perda, observamos que o modelo com `hidden_dim=8` (Loss: {{loss_epoch_100_hd8}}) apresentou uma perda significativamente menor na época 100 do que o modelo com `hidden_dim=16` (Loss: {{loss_epoch_100_hd16}}).\n",
        "\n",
        "Um aumento no número de neurônios na camada oculta (`hidden_dim`) geralmente confere ao modelo uma maior capacidade de representação e complexidade. No entanto, no início do treinamento (como na época 100), um modelo mais complexo pode ter uma performance pior por algumas razões:\n",
        "\n",
        "1.  **Inicialização aleatória**: Com mais parâmetros, há uma chance maior de que a inicialização aleatória dos pesos coloque o modelo em um ponto desfavorável no espaço de busca de pesos.\n",
        "2.  **Velocidade de convergência**: Modelos mais complexos podem exigir mais épocas para convergir, ou seja, para que seus pesos se ajustem adequadamente e a perda comece a diminuir consistentemente. A capacidade extra pode levar mais tempo para ser aproveitada.\n",
        "3.  **Overfitting inicial**: Embora o overfitting seja mais comum em estágios posteriores do treinamento, um modelo muito complexo pode começar a se ajustar ao ruído nos dados de treinamento mais rapidamente, impedindo que a perda geral diminua de forma eficiente nas primeiras épocas, especialmente se os dados de treinamento forem limitados ou ruidosos.\n",
        "\n",
        "Neste caso específico, na época 100, o modelo mais simples (`hidden_dim=8`) parece ter se estabilizado mais rapidamente ou encontrado um vale mais raso na função de perda, resultando em um valor de perda menor. Isso sugere que, para a tarefa e os dados em questão, uma complexidade moderada já é suficiente ou que o modelo mais complexo ainda está em uma fase inicial de adaptação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e479f363"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Apresentar a análise final sobre a redução (ou não) da loss com o aumento de neurônios na camada escondida.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef8155c4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "A comparação dos valores de perda na época 100 revela que o modelo com `hidden_dim=8` obteve uma perda de 0.0495, enquanto o modelo com `hidden_dim=16` resultou em uma perda de 0.4835. Isso significa que, na época 100, o modelo mais simples (com 8 neurônios na camada oculta) apresentou um desempenho significativamente melhor, com uma perda muito menor, do que o modelo mais complexo (com 16 neurônios na camada oculta).\n",
        "\n",
        "O impacto do aumento do número de neurônios na camada oculta na perda na época 100 foi, neste caso, negativo. Embora mais neurônios geralmente confiram maior capacidade de representação, no início do treinamento, um modelo mais complexo pode apresentar pior performance devido a:\n",
        "1.  **Inicialização aleatória**: Mais parâmetros significam maior probabilidade de uma inicialização desfavorável.\n",
        "2.  **Velocidade de convergência**: Modelos mais complexos podem precisar de mais épocas para convergir e aproveitar sua capacidade extra.\n",
        "3.  **Overfitting inicial**: Pode começar a se ajustar ao ruído mais rapidamente, impedindo a diminuição eficiente da perda nas primeiras épocas.\n",
        "\n",
        "Neste cenário específico, o modelo mais simples (`hidden_dim=8`) parece ter se estabilizado mais rapidamente, atingindo um valor de perda menor na época 100.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   O valor da perda na época 100 para o modelo com `hidden_dim=8` foi de 0.0495.\n",
        "*   Um novo modelo foi treinado com `hidden_dim=16`, e o valor da perda na época 100 para este modelo foi de 0.4835.\n",
        "*   Na época 100, o modelo com `hidden_dim=8` apresentou uma perda significativamente menor (0.0495) em comparação com o modelo de `hidden_dim=16` (0.4835), indicando um desempenho superior do modelo mais simples neste estágio inicial de treinamento.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Em estágios iniciais de treinamento, modelos com menor complexidade (menos neurônios na camada oculta) podem convergir mais rapidamente ou encontrar soluções melhores para a função de perda do que modelos mais complexos, que podem precisar de mais épocas para otimizar seus parâmetros adicionais.\n",
        "*   Para um estudo mais aprofundado, seria interessante continuar o treinamento do modelo com `hidden_dim=16` por mais épocas e observar se sua perda eventualmente supera a do modelo com `hidden_dim=8`, ou se ele atinge um mínimo global mais baixo, o que indicaria que sua maior capacidade de representação foi eventualmente aproveitada.\n"
      ]
    }
  ]
}